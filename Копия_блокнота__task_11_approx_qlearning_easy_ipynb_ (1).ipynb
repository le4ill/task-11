{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yw7uPLkFI49w",
        "outputId": "4b41ff79-5722-4ea0-a379-a556943c6cd3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Selecting previously unselected package libfontenc1:amd64.\n",
            "(Reading database ... 123605 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libfontenc1_1%3a1.1.4-1build3_amd64.deb ...\n",
            "Unpacking libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Selecting previously unselected package libxfont2:amd64.\n",
            "Preparing to unpack .../1-libxfont2_1%3a2.0.5-1build1_amd64.deb ...\n",
            "Unpacking libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Selecting previously unselected package libxkbfile1:amd64.\n",
            "Preparing to unpack .../2-libxkbfile1_1%3a1.1.0-1build3_amd64.deb ...\n",
            "Unpacking libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Selecting previously unselected package x11-xkb-utils.\n",
            "Preparing to unpack .../3-x11-xkb-utils_7.7+5build4_amd64.deb ...\n",
            "Unpacking x11-xkb-utils (7.7+5build4) ...\n",
            "Selecting previously unselected package xfonts-encodings.\n",
            "Preparing to unpack .../4-xfonts-encodings_1%3a1.0.5-0ubuntu2_all.deb ...\n",
            "Unpacking xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Selecting previously unselected package xfonts-utils.\n",
            "Preparing to unpack .../5-xfonts-utils_1%3a7.7+6build2_amd64.deb ...\n",
            "Unpacking xfonts-utils (1:7.7+6build2) ...\n",
            "Selecting previously unselected package xfonts-base.\n",
            "Preparing to unpack .../6-xfonts-base_1%3a1.0.5_all.deb ...\n",
            "Unpacking xfonts-base (1:1.0.5) ...\n",
            "Selecting previously unselected package xserver-common.\n",
            "Preparing to unpack .../7-xserver-common_2%3a21.1.4-2ubuntu1.7~22.04.11_all.deb ...\n",
            "Unpacking xserver-common (2:21.1.4-2ubuntu1.7~22.04.11) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../8-xvfb_2%3a21.1.4-2ubuntu1.7~22.04.11_amd64.deb ...\n",
            "Unpacking xvfb (2:21.1.4-2ubuntu1.7~22.04.11) ...\n",
            "Setting up libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Setting up xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Setting up libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Setting up libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Setting up x11-xkb-utils (7.7+5build4) ...\n",
            "Setting up xfonts-utils (1:7.7+6build2) ...\n",
            "Setting up xfonts-base (1:1.0.5) ...\n",
            "Setting up xserver-common (2:21.1.4-2ubuntu1.7~22.04.11) ...\n",
            "Setting up xvfb (2:21.1.4-2ubuntu1.7~22.04.11) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "Starting virtual X frame buffer: Xvfb.\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# Проверяем, выполняется ли код в Google Colab, и настроено ли окружение\n",
        "if 'google.colab' in sys.modules and not os.path.exists('.setup_complete'):\n",
        "    # Загружаем и выполняем скрипт для настройки Colab из репозитория Yandex Data School\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n",
        "    # Создаем файл-маркер, чтобы избежать повторной настройки\n",
        "    !touch .setup_complete\n",
        "\n",
        "# Проверяем, доступен ли графический дисплей в системе\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
        "    # Если графический дисплей отсутствует, запускаем виртуальный дисплей с помощью xvfb\n",
        "    !bash ../xvfb start\n",
        "    # Устанавливаем переменную окружения DISPLAY для работы графических приложений\n",
        "    os.environ['DISPLAY'] = ':1'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFhmE0HxI49t"
      },
      "source": [
        "# Approximate q-learning (10 pts)\n",
        "\n",
        "In this notebook you will teach a __PyTorch__ neural network to do Q-learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "skMa7z5cI49x"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Настраиваем отображение графиков непосредственно в ноутбуке\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDbfOfDjI49y",
        "outputId": "bc72adff-f443-486a-ccbf-63bf8a629473",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 619
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Создаем окружение \"CartPole-v0\"\n",
        "env = gym.make(\"CartPole-v0\").env\n",
        "\n",
        "# Сбрасываем состояние окружения для начала новой игры\n",
        "env.reset()\n",
        "\n",
        "# Получаем количество возможных действий в среде\n",
        "n_actions = env.action_space.n\n",
        "\n",
        "# Получаем размерность наблюдений из среды\n",
        "state_dim = env.observation_space.shape\n",
        "\n",
        "# Отображаем текущее состояние окружения в виде изображения\n",
        "plt.imshow(env.render(\"rgb_array\"))\n",
        "\n",
        "# Закрываем окружение после завершения работы\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvUtcMzZI49y"
      },
      "source": [
        "# Approximate Q-learning: building the network\n",
        "\n",
        "To train a neural network policy one must have a neural network policy. Let's build it.\n",
        "\n",
        "\n",
        "Since we're working with a pre-extracted features (cart positions, angles and velocities), we don't need a complicated network yet. In fact, let's build something like this for starters:\n",
        "\n",
        "![img](https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/yet_another_week/_resource/qlearning_scheme.png)\n",
        "\n",
        "For your first run, please only use linear layers (`nn.Linear`) and activations. Stuff like batch normalization or dropout may ruin everything if used haphazardly.\n",
        "\n",
        "Also please avoid using nonlinearities like sigmoid & tanh: since agent's observations are not normalized, sigmoids might be saturated at initialization. Instead, use non-saturating nonlinearities like ReLU.\n",
        "\n",
        "Ideally you should start small with maybe 1-2 hidden layers with < 200 neurons and then increase network size if agent doesn't beat the target score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7cz-CEy0I49z"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CehGlT_1I49z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0291468-bc26-48bc-e587-0acc22d5b9c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# Создаем последовательную нейронную сеть\n",
        "network = nn.Sequential()\n",
        "\n",
        "# Первый полносвязный слой, преобразующий входное состояние в 128-мерный вектор\n",
        "network.add_module('layer1_1', nn.Linear(state_dim[0], 128))\n",
        "# Функция активации ReLU после первого слоя\n",
        "network.add_module('relu1_1', nn.ReLU())\n",
        "\n",
        "# Второй полносвязный слой, уменьшающий размерность до 64\n",
        "network.add_module('layer1_2', nn.Linear(128, 64))\n",
        "# Функция активации ReLU после второго слоя\n",
        "network.add_module('relu1_2', nn.ReLU())\n",
        "\n",
        "# Третий полносвязный слой, уменьшающий размерность до 32\n",
        "network.add_module('layer1_3', nn.Linear(64, 32))\n",
        "# Функция активации ReLU после третьего слоя\n",
        "network.add_module('relu1_3', nn.ReLU())\n",
        "\n",
        "# Выходной слой, преобразующий 32-мерный вектор в количество действий в среде\n",
        "network.add_module('output_layer', nn.Linear(32, n_actions))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3iFr8ELzI490"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Функция для выбора действия с использованием epsilon-жадной стратегии\n",
        "def get_action(state, epsilon=0):\n",
        "    \"\"\"\n",
        "    Выбирает действие с использованием epsilon-жадной политики.\n",
        "    Если случайное число < epsilon, выбирается случайное действие.\n",
        "    Иначе выбирается действие с наибольшим значением Q(s, a).\n",
        "\n",
        "    Параметры:\n",
        "    state (array): Текущее состояние среды.\n",
        "    epsilon (float): Вероятность выбора случайного действия (по умолчанию 0).\n",
        "\n",
        "    Возвращает:\n",
        "    int: Выбранное действие.\n",
        "    \"\"\"\n",
        "    # Преобразуем состояние в тензор с дополнительной размерностью для батча\n",
        "    state = torch.tensor(state[None], dtype=torch.float32)\n",
        "\n",
        "    # Получаем Q-значения для текущего состояния\n",
        "    q_values = network(state).detach().numpy()\n",
        "\n",
        "    # С вероятностью epsilon выбираем случайное действие\n",
        "    if np.random.rand() < epsilon:\n",
        "        action = np.random.randint(0, q_values.shape[1])\n",
        "    else:\n",
        "        # Иначе выбираем действие с максимальным Q-значением\n",
        "        action = np.argmax(q_values)\n",
        "\n",
        "    return int(action)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGIdi1fSI490",
        "outputId": "ee3328db-8917-429e-8c90-31fce1933ea6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-a9b96e187485>:2: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  assert tuple(network(torch.tensor([s]*3, dtype=torch.float32)).size()) == (3, n_actions), \"please make sure your model maps state s -> [Q(s,a0), ..., Q(s, a_last)]\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "e=0.0 tests passed\n",
            "e=0.1 tests passed\n",
            "e=0.5 tests passed\n",
            "e=1.0 tests passed\n"
          ]
        }
      ],
      "source": [
        "# Сбрасываем окружение и получаем начальное состояние\n",
        "s = env.reset()\n",
        "\n",
        "# Проверка: модель должна преобразовывать состояние s в Q-значения для всех действий\n",
        "assert tuple(network(torch.tensor([s]*3, dtype=torch.float32)).size()) == (3, n_actions), \\\n",
        "    \"please make sure your model maps state s -> [Q(s,a0), ..., Q(s, a_last)]\"\n",
        "\n",
        "# Проверка: последний слой сети должен быть линейным (без нелинейности)\n",
        "assert isinstance(list(network.modules())[-1], nn.Linear), \\\n",
        "    \"please make sure you predict q-values without nonlinearity (ignore if you know what you're doing)\"\n",
        "\n",
        "# Проверка: функция get_action должна возвращать целое число\n",
        "assert isinstance(get_action(s), int), \\\n",
        "    \"get_action(s) must return int, not %s. try int(action)\" % (type(get_action(s)))\n",
        "\n",
        "# Тестирование epsilon-жадной стратегии\n",
        "for eps in [0., 0.1, 0.5, 1.0]:\n",
        "    # Считаем частоты выбора действий при данном epsilon\n",
        "    state_frequencies = np.bincount(\n",
        "        [get_action(s, epsilon=eps) for i in range(10000)], minlength=n_actions\n",
        "    )\n",
        "\n",
        "    # Определяем лучшее действие (действие с наибольшей частотой выбора)\n",
        "    best_action = state_frequencies.argmax()\n",
        "\n",
        "    # Проверяем, что частота лучшего действия соответствует теоретическим ожиданиям\n",
        "    assert abs(state_frequencies[best_action] - 10000 * (1 - eps + eps / n_actions)) < 200\n",
        "\n",
        "    # Проверяем, что частоты остальных действий также соответствуют ожиданиям\n",
        "    for other_action in range(n_actions):\n",
        "        if other_action != best_action:\n",
        "            assert abs(state_frequencies[other_action] - 10000 * (eps / n_actions)) < 200\n",
        "\n",
        "    print('e=%.1f tests passed' % eps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9fiNWZKI491"
      },
      "source": [
        "### Q-learning via gradient descent\n",
        "\n",
        "We shall now train our agent's Q-function by minimizing the TD loss:\n",
        "$$ L = { 1 \\over N} \\sum_i (Q_{\\theta}(s,a) - [r(s,a) + \\gamma \\cdot max_{a'} Q_{-}(s', a')]) ^2 $$\n",
        "\n",
        "\n",
        "Where\n",
        "* $s, a, r, s'$ are current state, action, reward and next state respectively\n",
        "* $\\gamma$ is a discount factor defined two cells above.\n",
        "\n",
        "The tricky part is with  $Q_{-}(s',a')$. From an engineering standpoint, it's the same as $Q_{\\theta}$ - the output of your neural network policy. However, when doing gradient descent, __we won't propagate gradients through it__ to make training more stable.\n",
        "\n",
        "To do so, we shall use `x.detach()` function which basically says \"consider this thing constant when doingbackprop\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVFj3M10I491"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def compute_td_loss(states, actions, rewards, next_states, is_done, gamma=0.99, check_shapes=False):\n",
        "    \"\"\"\n",
        "    Вычисление TD-ошибки с использованием операций PyTorch.\n",
        "\n",
        "    Параметры:\n",
        "    states (array): Набор текущих состояний.\n",
        "    actions (array): Действия, выполненные в данных состояниях.\n",
        "    rewards (array): Вознаграждения, полученные за действия.\n",
        "    next_states (array): Следующие состояния после выполнения действий.\n",
        "    is_done (array): Флаг завершения эпизода (1, если эпизод завершён).\n",
        "    gamma (float): Коэффициент дисконтирования (по умолчанию 0.99).\n",
        "    check_shapes (bool): Проверка корректности размерностей тензоров (по умолчанию False).\n",
        "\n",
        "    Возвращает:\n",
        "    torch.Tensor: Значение функции потерь.\n",
        "    \"\"\"\n",
        "    # Преобразуем входные данные в тензоры\n",
        "    states = torch.tensor(states, dtype=torch.float32)  # [batch_size, state_size]\n",
        "    actions = torch.tensor(actions, dtype=torch.long)   # [batch_size]\n",
        "    rewards = torch.tensor(rewards, dtype=torch.float32)  # [batch_size]\n",
        "    next_states = torch.tensor(next_states, dtype=torch.float32)  # [batch_size, state_size]\n",
        "    is_done = torch.tensor(is_done, dtype=torch.uint8)  # [batch_size]\n",
        "\n",
        "    # Вычисляем Q-значения для всех действий в текущих состояниях\n",
        "    predicted_qvalues = network(states)\n",
        "\n",
        "    # Выбираем Q-значения для выполненных действий\n",
        "    predicted_qvalues_for_actions = predicted_qvalues[range(states.shape[0]), actions]\n",
        "\n",
        "    # Вычисляем Q-значения для всех действий в следующих состояниях\n",
        "    predicted_next_qvalues = network(next_states)\n",
        "\n",
        "    # Вычисляем V*(next_states) как максимум Q-значений по всем действиям\n",
        "    next_state_values = predicted_next_qvalues.max(dim=1)[0]\n",
        "    assert next_state_values.dtype == torch.float32\n",
        "\n",
        "    # Вычисляем целевые Q-значения для функции потерь\n",
        "    target_qvalues_for_actions = rewards + (1 - is_done.float()) * gamma * next_state_values\n",
        "\n",
        "    # Для терминальных состояний Q(s, a) = r(s, a)\n",
        "    target_qvalues_for_actions = torch.where(is_done, rewards, target_qvalues_for_actions)\n",
        "\n",
        "    # Среднеквадратичная ошибка между предсказанными и целевыми Q-значениями\n",
        "    loss = torch.mean((predicted_qvalues_for_actions - target_qvalues_for_actions.detach()) ** 2)\n",
        "\n",
        "    if check_shapes:\n",
        "        # Проверка: Q-значения для всех действий в следующих состояниях имеют размерность 2\n",
        "        assert predicted_next_qvalues.data.dim() == 2, \"make sure you predicted q-values for all actions in next state\"\n",
        "        # Проверка: V(s') вычислено как максимум по оси действий\n",
        "        assert next_state_values.data.dim() == 1, \"make sure you computed V(s') as maximum over just the actions axis and not all axes\"\n",
        "        # Проверка: целевые Q-значения являются вектором\n",
        "        assert target_qvalues_for_actions.data.dim() == 1, \"there's something wrong with target q-values, they must be a vector\"\n",
        "\n",
        "    return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mR9ZKqSRI492",
        "outputId": "5a9b4ab3-6a1d-46c5-ac5b-89a6042e4ab9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n",
            "<ipython-input-8-30488d12dd36>:29: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at ../aten/src/ATen/native/TensorCompare.cpp:530.)\n",
            "  target_qvalues_for_actions = torch.where(\n"
          ]
        }
      ],
      "source": [
        "# Проверка корректности работы compute_td_loss\n",
        "s = env.reset()\n",
        "a = env.action_space.sample()\n",
        "next_s, r, done, _ = env.step(a)\n",
        "loss = compute_td_loss([s], [a], [r], [next_s], [done], check_shapes=True)\n",
        "loss.backward()\n",
        "\n",
        "# Проверяем, что функция возвращает скалярную ошибку (усреднённую по батчу)\n",
        "assert len(loss.size()) == 0, \"you must return scalar loss - mean over batch\"\n",
        "\n",
        "# Проверяем, что ошибка является дифференцируемой по весам сети\n",
        "assert np.any(next(network.parameters()).grad.detach().numpy() != 0), \\\n",
        "    \"loss must be differentiable w.r.t. network weights\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eo6b0b4vI492"
      },
      "source": [
        "### Playing the game"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-u1vXYQII492"
      },
      "outputs": [],
      "source": [
        "opt = torch.optim.Adam(network.parameters(), lr=1e-4) # Оптимизатор для обновления параметров сети"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EbtIGLEuI493",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3dadd2d3-0583-4565-aa4c-9b1da5717bb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "def generate_session(env, t_max=1000, epsilon=0, train=False):\n",
        "    \"\"\"Играем в среде с использованием агента на основе Q-learning и, при необходимости, обучаем его.\"\"\"\n",
        "    total_reward = 0\n",
        "    s = env.reset()\n",
        "\n",
        "    for t in range(t_max):\n",
        "        a = get_action(s, epsilon=epsilon)  # Получаем действие с учётом epsilon-жадной стратегии\n",
        "        next_s, r, done, _ = env.step(a)  # Выполняем действие\n",
        "\n",
        "        if train:\n",
        "            # Обучение нейронной сети\n",
        "            opt.zero_grad()\n",
        "            compute_td_loss([s], [a], [r], [next_s], [done]).backward()\n",
        "            opt.step()\n",
        "\n",
        "        total_reward += r  # Суммируем награды\n",
        "        s = next_s  # Переходим в следующее состояние\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    return total_reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-79ol_HI493"
      },
      "outputs": [],
      "source": [
        "epsilon = 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jBBE6LhRI493",
        "outputId": "df8938fe-277e-476e-89f3-4f3efdb71820",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch #0\tmean reward = 13.500\tepsilon = 0.500\n",
            "epoch #1\tmean reward = 14.090\tepsilon = 0.495\n",
            "epoch #2\tmean reward = 13.300\tepsilon = 0.490\n",
            "epoch #3\tmean reward = 14.420\tepsilon = 0.485\n",
            "epoch #4\tmean reward = 14.890\tepsilon = 0.480\n",
            "epoch #5\tmean reward = 28.870\tepsilon = 0.475\n",
            "epoch #6\tmean reward = 34.310\tepsilon = 0.471\n",
            "epoch #7\tmean reward = 38.380\tepsilon = 0.466\n",
            "epoch #8\tmean reward = 50.590\tepsilon = 0.461\n",
            "epoch #9\tmean reward = 60.070\tepsilon = 0.457\n",
            "epoch #10\tmean reward = 80.700\tepsilon = 0.452\n",
            "epoch #11\tmean reward = 106.350\tepsilon = 0.448\n",
            "epoch #12\tmean reward = 128.100\tepsilon = 0.443\n",
            "epoch #13\tmean reward = 139.660\tepsilon = 0.439\n",
            "epoch #14\tmean reward = 165.310\tepsilon = 0.434\n",
            "epoch #15\tmean reward = 166.900\tepsilon = 0.430\n",
            "epoch #16\tmean reward = 200.520\tepsilon = 0.426\n",
            "epoch #17\tmean reward = 217.610\tepsilon = 0.421\n",
            "epoch #18\tmean reward = 227.490\tepsilon = 0.417\n",
            "epoch #19\tmean reward = 239.310\tepsilon = 0.413\n",
            "epoch #20\tmean reward = 293.820\tepsilon = 0.409\n",
            "epoch #21\tmean reward = 280.840\tepsilon = 0.405\n",
            "epoch #22\tmean reward = 288.260\tepsilon = 0.401\n",
            "epoch #23\tmean reward = 333.570\tepsilon = 0.397\n",
            "You Win!\n"
          ]
        }
      ],
      "source": [
        "# Запуск обучения агента с отображением результатов\n",
        "for i in range(1000):\n",
        "    session_rewards = [generate_session(env, epsilon=epsilon, train=True) for _ in range(100)]\n",
        "    print(\"epoch #{}\\tmean reward = {:.3f}\\tepsilon = {:.3f}\".format(i, np.mean(session_rewards), epsilon))\n",
        "\n",
        "    epsilon *= 0.99  # Постепенное уменьшение epsilon для уменьшения вероятности случайных действий\n",
        "    assert epsilon >= 1e-4, \"Make sure epsilon is always nonzero during training\"\n",
        "\n",
        "    if np.mean(session_rewards) > 300:\n",
        "        print(\"You Win!\")\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqGiLPzHI493"
      },
      "source": [
        "### How to interpret results\n",
        "\n",
        "\n",
        "Welcome to the f.. world of deep f...n reinforcement learning. Don't expect agent's reward to smoothly go up. Hope for it to go increase eventually. If it deems you worthy.\n",
        "\n",
        "Seriously though,\n",
        "* __ mean reward__ is the average reward per game. For a correct implementation it may stay low for some 10 epochs, then start growing while oscilating insanely and converges by ~50-100 steps depending on the network architecture.\n",
        "* If it never reaches target score by the end of for loop, try increasing the number of hidden neurons or look at the epsilon.\n",
        "* __ epsilon__ - agent's willingness to explore. If you see that agent's already at < 0.01 epsilon before it's is at least 200, just reset it back to 0.1 - 0.5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9VBohEBxI493"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}